---
title: "R Notebook"
output: html_notebook
---

```{r}
library(tidyverse)
library(ggplot2)
library(tidytext)
library(tm)
library(widyr)
library(wordcloud)
library(wordcloud2)
library(glue)
```

```{r}
# read in scraped and cleaned data
full_mpaa <- read_rds("../mpaa_rating_reasons_app/data/mpaa.rds")
```

```{r}
full_mpaa %>% 
  filter(rating == "G") %>% 
  n_distinct()
```


```{r}
col_pal <- c("G" = "#1A9850", 
             "PG" = "#D9EF8B", 
             "PG-13" = "#FEE08B", 
             "R" = "#F46D43", 
             "NC-17" = "#A50026")
```

```{r}
all_yr_rating <- expand_grid(rating = c("G", "PG", "PG-13", "R", "NC-17"), year = 1992:2022)

all_yr_rating
```


```{r}
full_mpaa %>% 
  group_by(year, rating) %>% 
  count(rating) %>% 
  ungroup() %>% 
  full_join(all_yr_rating) %>% 
  mutate(n = replace_na(n, 0)) %>% 
  ggplot(aes(x = year, y = n, color = rating)) +
  geom_line(size = 1.1) +
  scale_color_manual(values = col_pal,
                     breaks=c('G', 'PG', 'PG-13', 'R', 'NC-17')) +
  theme_bw() +
  theme(panel.grid.minor.x = element_blank()) +
  scale_x_continuous(breaks = c(1990, 1995, 2000, 2005, 2010, 2015, 2020)) +
  labs(y = "Number of Movies Rated",
       color = "Rating",
       x = "") 

  
```

```{r}
year_summary <- full_mpaa %>%
  group_by(year) %>%
  count(year) %>%
  ungroup()

year_summary %>% 
  slice_min(n, n=1)

year_summary %>% 
  slice_max(n, n=1)

round(mean(year_summary$n), 0)

```

```{r}
overall_mean_len <- full_mpaa %>% 
  filter(rating != "G") %>%
  mutate(overall = mean(str_count(reason,"\\W+")))
  
full_mpaa %>% 
  group_by(year, rating) %>% 
  summarize(mean_len = mean(str_count(reason,"\\W+"))) %>% 
  ungroup() %>% 
  full_join(all_yr_rating) %>% 
  filter(rating != "G") %>% 
  ggplot() +
  geom_line(aes(x = year, y = mean_len, color = rating),
            size = .75,
            ) +
  geom_point(aes(x = year, y = mean_len, color = rating),
            size = 1.5,
            ) +
  scale_color_manual(values = col_pal,
                     breaks=c('G', 'PG', 'PG-13', 'R', 'NC-17')) +
  geom_hline(data = overall_mean_len, aes(yintercept = overall))+
  theme_bw() +
  theme(panel.grid.minor.x = element_blank()) +
  scale_x_continuous(breaks = c(1990, 1995, 2000, 2005, 2010, 2015, 2020)) +
  scale_y_continuous(breaks = c(2,4,6,8,10,12),
                     limit = c(0, 12)) +
  labs(y = "Average Number of Words in Rating Reason",
       color = "Rating",
       x = "")
```

```{r}
# reasons with the most words
full_mpaa %>% 
  mutate(reason = str_replace(reason, "martialarts", "martial arts"),
         reason = str_replace(reason, "druguse", "drug use"),
         reason = str_replace(reason, "drugabuse", "drug abuse"),
         reason = str_replace(reason, "substanceabuse", "substance abuse"),
         reason = str_replace(reason, "substanceuse", "substance use"),
         reason = str_replace(reason, "scifi", "sci-fi")) %>% 
  filter(reason_len > 2) %>% 
  slice_max(reason_len, n = 3) 
```

```{r}
full_mpaa %>%
  filter(rating != "G") %>% 
  mutate(reason = str_replace(reason, "martialarts", "martial arts"),
         reason = str_replace(reason, "druguse", "drug use"),
         reason = str_replace(reason, "drugabuse", "drug abuse"),
         reason = str_replace(reason, "substanceabuse", "substance abuse"),
         reason = str_replace(reason, "substanceuse", "substance use"),
         reason = str_replace(reason, "scifi", "sci-fi")) %>%
  filter(reason_len > 2) %>%
  slice_min(reason_len, n = 2)

```


```{r}
full_mpaa %>% 
  filter(grepl("marijuana", reason))
```

Notes on dataframes needed and their distinct features

For ratings wordclouds
  -Use less restrictive set of stop words to allow more thorough comparison (e.g. brief likely shows up more in PG, throughout in r)
  -Compound words need to be hyphenated after tokenizing in the word column
  -Keep the reasons column, at end fix the compound words in the reasons column

For top words page
  -Remove additional stop words (some, content, brief, strong, thematic, images, material, elements, mild, throughout, references, scene, scenes, sequences, related, dialogue, partial) (what about pervasive and throughout?)
  -Do some word stem combining (sex/sexy/sexual/sexually/sexuality/, drug(s)/drug use, violence/violent)
  -This is allow top words by rating/year to be "meatier" and more meaningful
  -Compound words can have space put back after tokenizing
  -Do I need the reasons column?
  -Will need an overall count based on rating(s) and year(s) selected to know which words to populate
  -Will need a by rating/year count for each of those top words to do the graph
  -Maybe a call out number- could do average number of words (overall, per rating and/or year?), could do number of unique words used in that time frame (overall, per rating and/or year?)
  
For modifying phrases
  -Start with full_mpaa data frame
  -Add column with the modifying phrases (this should to be done reactively, not in global) based on regex pattern
  -THEN tokenize the phrase column (use same stop words as for word clouds, plus the word selected)
  -Want counts for phrases AND for individual words
  -Want to make sure the word chosen is clearly displayed somewhere on the page

```{r}
# Top Words dataframe

# define stop words
mpaa_stop_words2 <- tribble(
  ~word, ~lexicon,
  "rated", "CUSTOM",
  "pg", "CUSTOM",
  "pg13", "CUSTOM",
  "r",  "CUSTOM",
  "nc17", "CUSTOM",
  "for", "CUSTOM",
  "and", "CUSTOM",
  "a", "CUSTOM",
  "an", "CUSTOM",
  "of", "CUSTOM",
  "the", "CUSTOM",
  "including", "CUSTOM",
  "involving", "CUSTOM",
  "some", "CUSTOM",
  "content", "CUSTOM",
  "brief", "CUSTOM",
  "strong", "CUSTOM",
  "thematic", "CUSTOM",
  "images", "CUSTOM",
  "material", "CUSTOM",
  "elements", "CUSTOM",
  "mild", "CUSTOM",
  "throughout", "CUSTOM",
  "reference", "CUSTOM",
  "references", "CUSTOM",
  "scene", "CUSTOM",
  "scenes", "CUSTOM",
  "sequences", "CUSTOM",
  "situations", "CUSTOM",
  "related", "CUSTOM",
  "dialogue", "CUSTOM",
  "partial", "CUSTOM",
  "pervasive", "CUSTOM",
  "throughout", "CUSTOM"
)

# unnest with tidytext
top_unigrams <- full_mpaa %>% 
  unnest_tokens(word, reason, drop = FALSE) %>% 
  anti_join(mpaa_stop_words2)

# putting the space or dash back between the compound terms
top_unigrams <- top_unigrams %>% 
  mutate(word = str_replace(word, "martialarts", "martial arts"),
         word = str_replace(word, "druguse", "drug use"),
         word = str_replace(word, "drugabuse", "drugabuse"),
         word = str_replace(word, "substanceabuse", "substance abuse"),
         word = str_replace(word, "substanceuse", "substance use"),
         word = str_replace(word, "scifi", "sci-fi")
         )%>% 
  #and combind words with the same stem
  mutate(word = str_replace(word, "sex$", "sex*"),
         word = str_replace(word, "sexual$", "sex*"),
         word = str_replace(word, "sexually$", "sex*"),
         word = str_replace(word, "^sexuality", "sex*"),
         word = str_replace(word, "sexy$", "sex*"),
         word = str_replace(word, "drug$", "drug/substance use/abuse**"),
         word = str_replace(word, "drugs$", "drug/substance use/abuse**"),
         word = str_replace(word, "^substance", "drug/substance use/abuse**"),
         word = str_replace(word, "^drug use", "drug/substance use/abuse**"),
         word = str_replace(word, "^drug abuse", "drug/substance use/abuse**"),
         word = str_replace(word, "^substance use", "drug use/substance abuse**"),
         word = str_replace(word, "^substance abuse", "drug/substance use/abuse**"),
         word = str_replace(word, "violence$", "violence/violent"),
         word = str_replace(word, "^violent", "violence/violent")
         )
```

```{r}
# count words by year and rating
top_yr_rating_counts <- top_unigrams %>% 
  group_by(year, rating) %>% 
  count(word) %>% 
  ungroup()

top_yr_rating_counts
```

```{r}
top_total_counts <- top_yr_rating_counts  %>% 
  group_by(word) %>% 
  summarize(total = sum(n)) %>% 
  arrange(desc(total)) %>% 
  ungroup() %>% 
  slice_max(total, n = 10)

top_total_counts %>% 
  ggplot(aes(x = total, y = reorder(word, total), fill = word)) +
  geom_col(width = .1) +
  geom_point(aes(color = word),
             size = 4,
             show.legend = FALSE) +
  scale_fill_brewer(palette = "Set3") + 
  scale_colour_brewer(palette = "Set3")+
  theme_bw() +
  theme(panel.grid.minor.x = element_blank()) +
  labs(y = "Word(s)",
       color = "Words",
       x = "Total Rating Reasons")
```

```{r}
#plot top 10 words over time
top_yr_rating_counts %>% 
  filter(word %in% top_total_counts$word) %>% 
  group_by(year, word) %>% 
  summarize(totals = sum(n)) %>% 
  ungroup() %>% 
  ggplot(aes(x = year, y = totals, color = word)) +
  geom_line(size = 1) +
  scale_colour_brewer(palette="Set3") +
  theme_bw() +
  theme(panel.grid.minor.x = element_blank()) +
  scale_x_continuous(breaks = c(1990, 1995, 2000, 2005, 2010, 2015, 2020)) +
  # scale_y_continuous(breaks = c(2,4,6,8,10,12),
  #                    limit = c(0, 12)) +
  labs(y = "Total Rating Reasons",
       color = "Words",
       x = "")
 

```


Wordclouds
```{r}
#Wordcloud dataframe

# define stop words
mpaa_stop_words1 <- tribble(
  ~word, ~lexicon,
  "rated", "CUSTOM",
  "pg", "CUSTOM",
  "pg13", "CUSTOM",
  "r",  "CUSTOM",
  "nc17", "CUSTOM",
  "for", "CUSTOM",
  "and", "CUSTOM",
  "a", "CUSTOM",
  "an", "CUSTOM",
  "of", "CUSTOM",
  "the", "CUSTOM",
  "including", "CUSTOM",
  "involving", "CUSTOM"
)

# unnest with tidytext
wc_unigrams <- full_mpaa %>% 
  unnest_tokens(word, reason) %>% 
  anti_join(mpaa_stop_words1)
 

# for use in wordclouds, workaround needed for drug use and martial arts to keep them together
wc_unigrams <- wc_unigrams %>% 
  mutate(word = str_replace(word, "martialarts", "martial-arts"),
         word = str_replace(word, "druguse", "drug-use"),
         word = str_replace(word, "drugabuse", "drug-abuse"),
         word = str_replace(word, "substanceabuse", "substance-abuse"),
         word = str_replace(word, "substanceuse", "substance-use"),
         word = str_replace(word, "scifi", "sci-fi")
  )
```


```{r}
pg_unigrams <- full_unigrams %>% 
  filter(rating == "PG")

pg13_unigrams <- full_unigrams %>% 
  filter(rating == "PG-13")
```

```{r}
all_pg <- paste(pg_unigrams$word,
                collapse = " ")

all_pg13 <- paste(pg13_unigrams$word,
                  collapse = " ")

pg_pg13 <- c(all_pg, all_pg13)

pg_pg13_tdm <- TermDocumentMatrix(VCorpus(VectorSource(pg_pg13)))

colnames(pg_pg13_tdm) <- c("pg", "pg13")

pg_pg13_m <- as.matrix(pg_pg13_tdm)

comparison.cloud(pg_pg13_m, colors = c("orange", "blue"), max.words = 30)
```

```{r}
commonality.cloud(pg_pg13_m, colors = "steelblue1", max.words = 75)
```
